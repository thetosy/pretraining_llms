{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fbf66-c7d5-4323-8f8a-acda7a12f66b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "#!pip install -U git+https://github.com/EleutherAI/lm-evaluation-harness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b1cfc",
   "metadata": {},
   "source": [
    "Evaluate TinySolar-248m-4k on 5 questions from the **TruthfulQA MC2 task**. This is a multiple-choice question answering task that tests the model's ability to identify true statements. You can read more about the TruthfulQA benchmark in [this paper](https://arxiv.org/abs/2109.07958), and you can checkout the code for implementing the tasks at this [github repo](https://github.com/sylinrl/TruthfulQA).\n",
    "\n",
    "The code below runs only the TruthfulQA MC2 task using the LM Evaluation Harness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c2c258-98de-43c7-9b96-cce7a6f20024",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-24:00:11:35,229 INFO     [__main__.py:272] Verbosity set to INFO\n",
      "2024-07-24:00:11:35,392 INFO     [__init__.py:403] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.\n",
      "2024-07-24:00:11:40,249 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2024-07-24:00:11:40,250 INFO     [__main__.py:369] Selected Tasks: ['truthfulqa_mc2']\n",
      "2024-07-24:00:11:40,251 INFO     [evaluator.py:158] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-07-24:00:11:40,251 INFO     [evaluator.py:195] Initializing hf model, with arguments: {'pretrained': './models/upstage/TinySolar-248m-4k'}\n",
      "2024-07-24:00:11:40,253 WARNING  [logging.py:61] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-07-24:00:11:40,254 INFO     [huggingface.py:170] Using device 'cpu'\n",
      "Downloading readme: 100%|██████████████████| 9.59k/9.59k [00:00<00:00, 37.2MB/s]\n",
      "Downloading data: 100%|██████████████████████| 271k/271k [00:00<00:00, 1.52MB/s]\n",
      "Generating validation split: 100%|██| 817/817 [00:00<00:00, 97837.16 examples/s]\n",
      "2024-07-24:00:11:44,896 INFO     [evaluator.py:274] Setting fewshot random generator seed to 1234\n",
      "2024-07-24:00:11:44,898 INFO     [task.py:423] Building contexts for truthfulqa_mc2 on rank 0...\n",
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 760.80it/s]\n",
      "2024-07-24:00:11:44,906 INFO     [evaluator.py:457] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|███████████| 33/33 [01:58<00:00,  3.60s/it]\n",
      "2024-07-24:00:13:43,833 WARNING  [huggingface.py:1314] Failed to get model SHA for ./models/upstage/TinySolar-248m-4k at revision main. Error: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/upstage/TinySolar-248m-4k'. Use `repo_type` argument if needed.\n",
      "2024-07-24:00:13:44,813 INFO     [evaluation_tracker.py:269] Output path not provided, skipping saving results aggregated\n",
      "hf (pretrained=./models/upstage/TinySolar-248m-4k), gen_kwargs: (None), limit: 5.0, num_fewshot: None, batch_size: 1\n",
      "|    Tasks     |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|--------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|truthfulqa_mc2|      2|none  |     0|acc   |↑  |0.4008|±  |0.2446|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=./models/upstage/TinySolar-248m-4k \\\n",
    "    --tasks truthfulqa_mc2 \\\n",
    "    --device cpu \\\n",
    "    --limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeef916",
   "metadata": {},
   "source": [
    "### Evaluation for the Hugging Face Leaderboard\n",
    "Run the code below to test your own model against the evaluations required for the [Hugging Face leaderboard](https://huggingface.co/open-llm-leaderboard). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc097be-7dbb-4a90-a954-f39d30e8e52c",
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def h6_open_llm_leaderboard(model_name):\n",
    "  task_and_shot = [\n",
    "      ('arc_challenge', 25),\n",
    "      ('hellaswag', 10),\n",
    "      ('mmlu', 5),\n",
    "      ('truthfulqa_mc2', 0),\n",
    "      ('winogrande', 5),\n",
    "      ('gsm8k', 5)\n",
    "  ]\n",
    "\n",
    "  for task, fewshot in task_and_shot:\n",
    "    eval_cmd = f\"\"\"\n",
    "    lm_eval --model hf \\\n",
    "        --model_args pretrained={model_name} \\\n",
    "        --tasks {task} \\\n",
    "        --device cpu \\\n",
    "        --num_fewshot {fewshot}\n",
    "    \"\"\"\n",
    "    os.system(eval_cmd)\n",
    "\n",
    "h6_open_llm_leaderboard(model_name=\"YOUR_MODEL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
